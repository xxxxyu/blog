<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://xxxxyu.github.io/blog/ name=base><title>
Xyu's Blog • Vision-Language-Action (VLA) Models: A Review of Recent Progress</title><link href=https://xxxxyu.github.io/blog/img/xyu_logo.png rel=icon type=image/png><link title="Xyu's Blog - Atom Feed" href=https://xxxxyu.github.io/blog/atom.xml rel=alternate type=application/atom+xml><link href="https://xxxxyu.github.io/blog/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://xxxxyu.github.io/blog/main.css?h=4fac61238e0c1fab5df1" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Recent VLAs evolve from discrete to continuous, and from single-system (system 1 only) to dual-system." name=description><meta content="Recent VLAs evolve from discrete to continuous, and from single-system (system 1 only) to dual-system." property=og:description><meta content="Vision-Language-Action (VLA) Models: A Review of Recent Progress" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://xxxxyu.github.io/blog/posts/vla-models-review/ property=og:url><meta content="Xyu's Blog" property=og:site_name><noscript><link href=https://xxxxyu.github.io/blog/no_js.css rel=stylesheet></noscript><script src=https://xxxxyu.github.io/blog/js/initializeTheme.min.js></script><script defer src=https://xxxxyu.github.io/blog/js/themeSwitcher.min.js></script><script src="https://xxxxyu.github.io/blog/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://xxxxyu.github.io/blog/>Xyu's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/posts/>Posts </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/archive/>Archive </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/tags/>Tags </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/poems/>Poems </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/academic/>Academic </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">Vision-Language-Action (VLA) Models: A Review of Recent Progress</h1><a class="u-url u-uid" href=https://xxxxyu.github.io/blog/posts/vla-models-review/></a><ul class=meta><li>By <span class=p-author>Xiangyu Li</span></li><span class="hidden p-author h-card"> <a title="Xiangyu Li" class=u-url href=https://xxxxyu.github.io/blog/ rel=author>Xiangyu Li</a> </span><li><time class=dt-published datetime=2025-09-16><span aria-hidden=true class=separator>•</span>16th Sep 2025</time><li title="1435 words"><span aria-hidden=true class=separator>•</span>8 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/review/>Review</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/vla/>VLA</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/embodied-ai/>Embodied AI</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2025-09-16>Updated on 16th Sep 2025</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/xxxxyu/blog/commits/main/content/posts/vla-models-review/index.md>See changes</a></ul><p class=p-summary hidden>Recent VLAs evolve from discrete to continuous, and from single-system (system 1 only) to dual-system.<section class="e-content body"><blockquote><p>I am new to this field — feel free to discuss, and welcome to bring up any questions! This is adapted from my slides, available at: <a href=https://xxxxyu.github.io/blog/posts/vla-models-review/resources/vla_review_0911.pdf>[PDF]</a>.</blockquote><h2 id=background-and-concepts>Background and Concepts</h2><h3 id=the-concept-of-vision-language-action-vla-models>The Concept of Vision-Language-Action (VLA) Models</h3><p>According to my understanding, <em>Vision-Language-Action (VLA)</em> Models are multi-modal foundation models for embodied AI, which ingest <strong>vision</strong> (e.g., observations in video streams) and <strong>language</strong> (e.g., user instructions) as inputs, and generate low-level robot <strong>actions</strong> (i.e., the <em>control policy</em>) as outputs. Mechanically, a VLA utilizes a <em>VLM (Vision-Language Model)</em> for <em>VL-conditioned action generation</em>.</p><img alt="The concepts and timelines related to the development of VLA." src="https://xxxxyu.github.io/blog/img/vla-models-review/timeline.png?h=402c8c050567a03dd024" class=dimmable-image height=404 loading=lazy width=1732><h3 id=add-vlm-based-task-planners-for-long-horizon-tasks>Add VLM-Based Task Planners for Long-Horizon Tasks</h3><p>VLAs are initially optimized for the low-level robot control policy, which is insufficient for completing complex and long-horizon tasks without end-to-end training. An effective approach is to add an LLM/VLM-based <em>task planner</em> to decompose a long-horizon task into simple subtasks, so that the VLA could complete them one by one. While earlier works usually adopt a separate model as the task planner, recent works are utilizing a shared VLM backbone for both task planning and control policy within the same model (i.e., the <em>dual-system</em> design).</p><img alt="An example of a hierarchical robot policy (high-level planning + low-level control)." src="https://xxxxyu.github.io/blog/img/vla-models-review/hierarchical_policy.png?h=0b6037ee70219e197e7e" class=dimmable-image height=404 loading=lazy width=1672><h2 id=recent-progress-of-vla>Recent Progress of VLA</h2><p>I summarize the trend of recent VLA as evolving <strong>from <em>system-1-only</em> (control) to <em>dual-system</em> (planning + control), and from <em>discrete</em> action to <em>continuous</em> action.</strong> I divide recent progress of VLA into 4 quadrants:</p><img alt="The quandrants of recent VLAs." src="https://xxxxyu.github.io/blog/img/vla-models-review/quadrants.png?h=ff98285742f0d10adf07" class=invertible-image height=754 loading=lazy width=1890><p>In the following of this section, I’ll introduce these categories respectively.<h3 id=discrete-vla>Discrete VLA</h3><p><em>Discrete VLA</em> generates <em>discrete action tokens</em>. Specifically, it adopts <em>discrete action tokenization</em> that maps low-level robot actions to discrete tokens, and trains the VLM to generate such tokens <em>autoregressively</em>, just like generating text tokens. This provides a straightforward approach to align the two modalities, action and language, which simplifies the training based on autoregressive VLMs (i.e., from next token prediction to next action prediction). However, it suffers from high latency and low FPS in robot control, since the autoregressive generation paradigm needs to go through the entire VLA for each forward pass.<p>Some representative methods:<ul><li><strong>RT-2</strong><sup class=footnote-reference id=fr-2-1><a href=#fn-2>[1]</a></sup> (ViT + PALI-X/PALM-E): a pioneer work that proposes and popularizes the term “VLA”.<li><strong>OpenVLA</strong><sup class=footnote-reference id=fr-3-1><a href=#fn-3>[2]</a></sup> (DinoV2 & SigLIP + Llama2 7B): an influential open-source VLA model (3.8k stars on <a href=https://github.com/openvla/openvla>GitHub</a>).<li><strong>FAST</strong><sup class=footnote-reference id=fr-4-1><a href=#fn-4>[3]</a></sup>: an action tokenizer that compresses action sequences with DCT (Discrete Cosine Transform).</ul><img alt="Overview of OpenVLA." src="https://xxxxyu.github.io/blog/img/vla-models-review/openvla.png?h=166905ad3eb3f8cf9378" class=dimmable-image height=400 loading=lazy width=1072><h3 id=continuous-vla>Continuous VLA</h3><p><em>Continuous VLA</em> samples from a <em>continuous action space</em>, which allows smoother control with higher precision, but also increases the difficulty of training atop existing language models. To solve this, Physical Intelligence first proposes to integrate a <em>flow-matching (a variant of diffusion) action expert</em> to a pre-trained VLM, and trains $\pi_0$<sup class=footnote-reference id=fr-5-1><a href=#fn-5>[4]</a></sup> atop a pre-trained PaliGemma 2B VLM.<p>The insight is to 1) utilize the VLM pre-trained on <em>internet-scale</em> datasets for <strong>semantic understanding and generalization</strong>, and 2) utilize the flow-matching action expert trained on <em>cross-embodiment</em> datasets for <strong>high-frequency (up to 50Hz) control policy</strong>. It also allows optional post-training fine-tuning for difficult or unseen tasks.</p><img alt="Overview of $\pi_0$." src="https://xxxxyu.github.io/blog/img/vla-models-review/pi0.png?h=bbf36e0647c609c73a63" class=dimmable-image height=402 loading=lazy width=1384><p>Similarly, NVIDIA Isaac trains GR00T N1(.5)<sup class=footnote-reference id=fr-6-1><a href=#fn-6>[5]</a></sup> that combines an pre-trained Eagle-2 VLM and an diffusion-based action head, which is the first foundation model for generalist humanoid robots. In both $\pi_0$ and GR00T, the VLM backbone and action expert communicates through attention modules, so that the generated actions are conditioned on the hidden states (i.e., KV) of the VLM. Still, there are two technical differences:<ul><li><strong>Attention mechanism</strong>: $\pi_0$ concatenates the VL and action KV and conducts masked self-attention (a <a href=https://huggingface.co/blog/pi0>blog</a> illustrates this clearly); GR00T directly conducts cross-attention between the two parts.<li><strong>Number of VLM layers involved</strong>: $\pi_0$ aligns the number of layers in the action expert to the VLM backbone, and conducts self-attention in each layer (MoE-like); GR00T only keeps the hidden states of the last layer in the VLM<sup class=footnote-reference id=fr-7-1><a href=#fn-7>[6]</a></sup>, and conducts cross-attention with it for each layer.</ul><h3 id=dual-system-vla>Dual-System VLA</h3><p>Different from earlier works that use a separate LLM/VLM as the task planner (i.e., system-1-only VLA), <em>dual-system VLA</em> utilize the VLM backbone in VLA as the task planner, so the system 2 (high-level planning) and system 1 (low-level control policy) <strong>shares one VLM</strong>. This further enhances open-world generalization of the VLA, by learning to <strong>predict subtasks from user instructions by itself</strong>. Besides, it reduces the resource requirements compared to using a separate task planner model.<blockquote><p>Question: does it also help improve model performance, as the system 1 and 2 are better aligned? On the other hand, does this cause potential interference between different objectives?</blockquote><img alt="Overview of $\pi_{0.5}$." src="https://xxxxyu.github.io/blog/img/vla-models-review/pi05.png?h=8888c95b1059aed475be" class=dimmable-image height=600 loading=lazy width=1480><p>$\pi_{0.5}$<sup class=footnote-reference id=fr-8-1><a href=#fn-8>[7]</a></sup> is the first of this category, trained by Physical Intelligence. Compared to $\pi_0$, it involves new training data, including object detection, instructions & subtask commands, discrete actions, etc. At inference time, it first predicts low-level command from high-level prompt with the VLM (system 2), then executes the low-level command with the VLM and action expert (system 1). This paradigm (training recipe and inference scheme) is followed by recent VLA like G0<sup class=footnote-reference id=fr-9-1><a href=#fn-9>[8]</a></sup> by Galaxea and WALL-OSS<sup class=footnote-reference id=fr-10-1><a href=#fn-10>[9]</a></sup> by X Square Robot. While most of these models are continuous VLA, WALL-OSS also includes a discrete version with FAST tokenization (<a href=https://huggingface.co/x-square-robot/wall-oss-fast>WALL-OSS-FAST</a>).<p>Their repositories and open-source states:<ul><li>$\pi_{0.5}$: Weights opened. Code partially opened at <a href=https://github.com/Physical-Intelligence/openpi>Physical-Intelligence/openpi</a>. Inference code for the VLM subtask prediction is not opened.<li>G0: Weights and open-world dataset opened. Code partially opened at <a href=https://github.com/OpenGalaxea/G0>OpenGalaxea/G0</a>. Currently only support real-robot inference.<li>WALL-OSS: Weights and code opened at <a href=https://github.com/X-Square-Robot/wall-x>X-Square-Robot/wall-x</a>.</ul><h2 id=summary-and-future-look>Summary and Future Look</h2><p>In the past 3 years (from RT-2 in 2023), VLA has rapidly evolved from discrete to continuous, and from single-system to dual-system. In the following years, I personally think <em>native multi-tasking</em> will be another trend of VLA (I will probably write another post) — embodied agents should be capable of performing multiple fundamentally different tasks (e.g., chat, memory, navigation) instead of restricted to “action”. As introduced above, recent models are already sharing the Internet-scale pre-trained VLM backbone for task planning and control policy (though still restricted to action tasks), which lays the foundation for more aggressive model sharing (one VLM backbone for multiple tasks) as a step forward in the future.<p>I am currently working on building this <em>multi-expert foundation model</em> for native multi-tasking of embodied agents — feel free to contact for discussion and collaboration!<hr><ol class=footnotes-list><li id=fn-2><p>Zitkovich, Brianna, et al. “RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.” <em>Conference on Robot Learning</em>. PMLR, 2023. <a href=#fr-2-1>↩</a></p><li id=fn-3><p>Kim, Moo Jin, et al. “OpenVLA: An Open-Source Vision-Language-Action Model.” <em>arXiv preprint arXiv:2406.09246</em> (2024). <a href=#fr-3-1>↩</a></p><li id=fn-4><p>Pertsch, Karl, et al. “FAST: Efficient Action Tokenization for Vision-Language-Action Models.” <em>arXiv preprint arXiv:2501.09747</em> (2025). <a href=#fr-4-1>↩</a></p><li id=fn-5><p>Black, Kevin, et al. “$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control.” <em>arXiv preprint arXiv:2410.24164</em> (2024). <a href=#fr-5-1>↩</a></p><li id=fn-6><p>Bjorck, Johan, et al. “GR00T N1: An Open Foundation Model for Generalist Humanoid Robots.” <em>arXiv preprint arXiv:2503.14734</em> (2025). <a href=#fr-6-1>↩</a></p><li id=fn-7><p>Specifically, the language backbone of the VLM in GR00T N1.5 is fine-tuned from the first 14 layers of the pre-trained <a href=https://huggingface.co/Qwen/Qwen3-1.7B>Qwen3-1.7B</a> (28 layers in total), according to my test of similarity between the weights. <a href=#fr-7-1>↩</a></p><li id=fn-8><p>Intelligence, Physical, et al. “$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization.” <em>arXiv preprint arXiv:2504.16054</em> (2025). <a href=#fr-8-1>↩</a></p><li id=fn-9><p>Jiang, Tao, et al. “Galaxea Open-World Dataset and G0 Dual-System VLA Model.” <em>arXiv preprint arXiv:2509.00576</em> (2025). <a href=#fr-9-1>↩</a></p><li id=fn-10><p>X Square Robot. “WALL-OSS: Igniting VLMs toward the Embodied Space.” 2025, <a href=https://x2robot.cn-wlcb.ufileos.com/wall_oss.pdf>https://x2robot.cn-wlcb.ufileos.com/wall_oss.pdf</a>. White paper. <a href=#fr-10-1>↩</a></p></ol></section><nav class="full-width article-navigation"><div><a aria-describedby=left_title aria-label=Prev href=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/><span class=arrow>←</span> Prev</a><p aria-hidden=true id=left_title>Reading Notes of Dario Amodei’s Blog</div><div><a aria-describedby=right_title aria-label=Next href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/>Next <span class=arrow>→</span></a><p aria-hidden=true id=right_title>Enhancing GPTQv2 Format Support in vLLM: Analysis and Implementation</div></nav></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#background-and-concepts>Background and Concepts</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#the-concept-of-vision-language-action-vla-models>The Concept of Vision-Language-Action (VLA) Models</a><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#add-vlm-based-task-planners-for-long-horizon-tasks>Add VLM-Based Task Planners for Long-Horizon Tasks</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#recent-progress-of-vla>Recent Progress of VLA</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#discrete-vla>Discrete VLA</a><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#continuous-vla>Continuous VLA</a><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#dual-system-vla>Dual-System VLA</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vla-models-review/#summary-and-future-look>Summary and Future Look</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://xxxxyu.github.io/blog/katex.min.css rel=stylesheet><script defer src=https://xxxxyu.github.io/blog/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://xxxxyu.github.io/blog/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://xxxxyu.github.io/blog/atom.xml> <img alt=feed loading=lazy src=https://xxxxyu.github.io/blog/social_icons/rss.svg title=feed> </a><li class=js><a class="nav-links no-hover-padding social" data-encoded-email=eGlhbmd5dS5zZGxjQGZveG1haWwuY29t href=#><img alt=email loading=lazy src=https://xxxxyu.github.io/blog/social_icons/email.svg title=email> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/xxxxyu/> <img alt=github loading=lazy src=https://xxxxyu.github.io/blog/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" href="https://scholar.google.com/citations?user=IjoWeIMAAAAJ&hl=en" rel=" me"> <img alt=scholar loading=lazy src=https://xxxxyu.github.io/blog/social_icons/scholar.svg title=scholar> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://space.bilibili.com/244315926/> <img alt=bilibili loading=lazy src=https://xxxxyu.github.io/blog/social_icons/bilibili.svg title=bilibili> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>Xyu’s Blog © 2026 Xiangyu Li • Unless otherwise noted, the content in this website is available under the <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a> license.</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> • <a href=https://github.com/xxxxyu/blog> Site source </a></small></div></section><script async src=https://xxxxyu.github.io/blog/js/decodeMail.min.js></script><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>