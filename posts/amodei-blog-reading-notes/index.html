<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://xxxxyu.github.io/blog/ name=base><title>
Xyu's Blog • Reading Notes of Dario Amodei's Blog</title><link href=https://xxxxyu.github.io/blog/img/xyu_logo.png rel=icon type=image/png><link title="Xyu's Blog - Atom Feed" href=https://xxxxyu.github.io/blog/atom.xml rel=alternate type=application/atom+xml><link href="https://xxxxyu.github.io/blog/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://xxxxyu.github.io/blog/main.css?h=4fac61238e0c1fab5df1" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reading Notes of Dario Amodei's Blog." name=description><meta content="Reading Notes of Dario Amodei's Blog." property=og:description><meta content="Reading Notes of Dario Amodei's Blog" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/ property=og:url><meta content="Xyu's Blog" property=og:site_name><noscript><link href=https://xxxxyu.github.io/blog/no_js.css rel=stylesheet></noscript><script src=https://xxxxyu.github.io/blog/js/initializeTheme.min.js></script><script defer src=https://xxxxyu.github.io/blog/js/themeSwitcher.min.js></script><script src="https://xxxxyu.github.io/blog/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://xxxxyu.github.io/blog/>Xyu's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/posts/>Posts </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/archive/>Archive </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/tags/>Tags </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/poems/>Poems </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/academic/>Academic </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">Reading Notes of Dario Amodei’s Blog</h1><a class="u-url u-uid" href=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/></a><ul class=meta><li>By <span class=p-author>Xiangyu Li</span></li><span class="hidden p-author h-card"> <a title="Xiangyu Li" class=u-url href=https://xxxxyu.github.io/blog/ rel=author>Xiangyu Li</a> </span><li><time class=dt-published datetime=2025-08-02><span aria-hidden=true class=separator>•</span>2nd Aug 2025</time><li title="1649 words"><span aria-hidden=true class=separator>•</span>9 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/essay/>Essay</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/reading/>Reading</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2025-08-04>Updated on 4th Aug 2025</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/xxxxyu/blog/commits/main/content/posts/amodei-blog-reading-notes/index.md>See changes</a></ul><p class=p-summary hidden>Reading Notes of Dario Amodei's Blog.<section class="e-content body"><blockquote><p>This is still in progress — but existing contents are already readable!</blockquote><p>Recently I’ve read some blog posts<sup class=footnote-reference id=fr-1-1><a href=#fn-1>[1]</a></sup> of <a href=https://www.darioamodei.com/>Dario Amodei</a>, the CEO of <a href=https://www.anthropic.com/>Anthropic</a>, and find them very insightful and inspiring to me — a Ph.D. candidate in AI-related fields, and also a deep user of Anthropic’s Claude models.<p>Specifically, they broaden my view both academically and industrially, with an interdisciplinary perspective. So this is to write down the thoughts that came to me while reading, to not let them slip away.<p>Amodei’s posts covered (in my reading order):<ul><li><a href=https://www.darioamodei.com/post/on-deepseek-and-export-controls><em>On DeepSeek and Export Controls</em></a> (Jan. 2025)<li><a href=https://www.darioamodei.com/post/the-urgency-of-interpretability><em>The Urgency of Interpretability</em></a> (Apr. 2025)<li><a href=https://www.darioamodei.com/essay/machines-of-loving-grace><em>Machines of Loving Grace</em></a> (Oct. 2024)</ul><h2 id=envisioning-powerful-ai>Envisioning Powerful AI</h2><p>In <a href=https://www.darioamodei.com/essay/machines-of-loving-grace><em>Machines of Loving Grace</em></a>, Amodei discusses a lot about what would <em>“powerful AI”</em> (he uses this phrase as he dislike the term <em>AGI</em>) be like in the future, and how will it change every aspect of our human society in the 5-10 years after its emergence (<em>“it could come as early as 2026”</em> as Amodei predicts). In Amodei’s vision, the powerful AI, which is similar to today’s LLMs in form, but not necessarily the same in implementation, has the following properties in my summarization:<ul><li><strong>Super intelligent.</strong> It is <em>“smarter than a Nobel Prize winner”</em> in terms of pure intelligence. For example, it can prove unsolved mathematical theorems and write extremely good novels.<li><strong>Works with interfaces.</strong> It has all the <em>“interfaces”</em> a human would require to work virtually with, other than just chatting.<li><strong>Autonomous.</strong> It can autonomously solve given tasks for a long period, with only necessary clarification from human.<li><strong>Interacts virtually.</strong> It doesn’t have a physical embodiment, but can control (and even create) physical tools through a computer.<li><strong>Massive amount and high speed.</strong> It has abundant resources (repurposed from training) for massive deployment (e.g., millions of instances), and learns and generates actions much faster than human (e.g., by 10-100×).<li><strong>Independent yet cooperative.</strong> The millions of instances can work both independently on unrelated tasks, and cooperatively on one task, with some of them fine-tuned to be especially good at particular tasks (I would call them <em>experts</em>).</ul><p>He summarizes this as a <em>“country of geniuses in a datacenter”</em>, which is principally capable of solving very difficult problems very fast, but still limited by several factors from the real world. For example, speed of the outside world, need for data (e.g., in particle physics), intrinsic complexity (e.g., <a href=https://en.wikipedia.org/wiki/Three-body_problem><em>the three-body problem</em></a>), constraints from humans (e.g., by laws and people’s willingness), and physical laws (this even limits the density of the digital world by limiting number of transistors per square centimeter and minimum energy per bit erased).<p>Therefore, he proposes that we should be talking about <em>“the marginal returns to intelligence”</em>, which is borrowed from the economic term, to conceptualize a world with very powerful AI. And all his visions are based on this, with awareness of in what areas, to what extent, and on what timescale does very powerful AI helps. This is a refreshing idea to me — we could easily end up with either overly optimistic or overly pessimistic imaginations without a reasonable measurement of the impact of intelligence.<p>The above basic assumptions and framework<sup class=footnote-reference id=fr-2-1><a href=#fn-2>[2]</a></sup>, although not the main part of <em>Machines of Loving Grace</em>, impress me the most. After all, the hardest thing is not to infer a reasonable answer given a solid framework, but to propose the framework.<p>He spends the rest of the essay to elaborate on what would happen 5-10 years after the powerful AI comes, in five main aspects: biology and health, neuroscience and mind, economic development and poverty, peace and governance, work and meaning. In short, his basic prediction is that <strong>powerful AI will allow us to compress the progress that human would have achieved over the next 50-100 years into 5-10 years</strong>, which he refers to as the <em>“compressed 21st century”</em>. The question is when will that powerful AI actually come.<p>In contrast to the <a href=https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework><em>Basic assumptions and framework</em></a> section, i think some points in the prediction sections, for example, the “liberal democracy vs. authoritarianism” part<sup class=footnote-reference id=fr-3-1><a href=#fn-3>[3]</a></sup>, are rather personal. It is clear that even within the same analytical framework, different people would have different understanding and imagination on certain issues, based on their unique background and characteristics. That’s an important reason why I like to read stuffs by people of different backgrounds, and now I am attempting to keep notes.<h2 id=mechanistic-interpretability-for-safe-ai>Mechanistic Interpretability for Safe AI</h2><p>As modern AI becomes more and more powerful, towards the “powerful AI” vision discussed above, the question of how to govern powerful AI and ensure it’s <em>safe</em> is gaining increasing importance (and the definition of safety is another big question). Technically, some safety mechanisms must be developed for a powerful AI system in the future (when it becomes powerful enough), and interpretability is definitely one of the most important aspects for safety control. In <a href=https://www.darioamodei.com/post/the-urgency-of-interpretability><em>The Urgency of Interpretability</em></a>, Amodei discusses the importance and history of mechanistic interpretability, and Anthropic’s recent progress on this.<p>In my understanding, the reason why interpretability is so important is that unlike conventional systems, which are programed in a definite manner, modern AI (i.e., AI based on deep neural networks) is mostly a <em>blackbox</em> — even people who trained them cannot have a 100% (sometimes not even 1%) clear understanding of why and how they produce certain outputs against some inputs. As Amodei quoted from <a href=https://colah.github.io/about.html>Chris Olah</a>, his co-founder, generative AI systems are <em>“grown”</em> (I like this word) more than they are <em>“built”</em> — their internal mechanisms are <em>emergent</em> rather than directly designed. Unfortunately, it is very possible that such “blackbox” (for now) systems will have increasingly huge impact on us humans, until one day it must be transparent and interpretable enough for more various applications.<p>Back to <em>mechanistic interpretability</em>, it is a field of <em>reverse-engineering</em> neural networks to understand the specific algorithms and computational mechanisms they learn to implement internally. Different from other interpretability approaches that focus on input-output relationships or high-level model behavior, mechanistic interpretability seeks detailed, mechanistic understanding of the system at low level (e.g., in layers and neurons). It was first studied on CNNs, and now LLMs. Chris Olah, co-founder of Anthropic, has been working on this field since he was at Google and OpenAI.<p>Given the example of a LLM, the first step is to find single interpretable neurons<sup class=footnote-reference id=fr-4-1><a href=#fn-4>[4]</a></sup>, and then they found that while some neurons were immediately interpretable, the vast majority were an incoherent pastiche of many different words and concepts, which allows the model to express more concepts than it has neurons. They referred to this phenomenon as <em>superposition</em>. To interpret the more complicated superpositions, they adopted a signal-processing-inspired method called <em>sparse autoencoders</em><sup class=footnote-reference id=fr-5-1><a href=#fn-5>[5]</a></sup>, with which they were able to find combinations of neurons that did correspond to cleaner, more human-understandable concepts (even very subtle ones like the concept of “literally or figuratively hedging or hesitating”), which they call <em>features</em>. They were able to find over 30 million features in Claude 3 Sonnet, Anthropic’s medium-sized commercial model, through this method.<p>More could be done to understand the model’s working mechanism after finding the features. For example, artificially amplifying neurons related to a feature would significantly affect the model’s behavior (<a href=https://www.anthropic.com/news/golden-gate-claude>“Golden State Claude”</a>); tracking and manipulating groups of features (i.e., <em>circuits</em><sup class=footnote-reference id=fr-6-1><a href=#fn-6>[6]</a></sup>) could help to understand the model’s reasoning process. Anthropic’s research blogs <sup class=footnote-reference id=fr-7-1><a href=#fn-7>[7]</a></sup> introduce these progresses in more details.<p>I appreciate the idea of mechanistic interpretability to “reverse engineer” deep neural networks, including modern LLMs with billions of parameters, as it provides valuable insights about how the AI models works inside. It seems on a right track but I doubt if it is the ultimate path towards powerful and safe AI, to catch “bugs” inside a huge neural network — it would be like eliminating cancer cells of a blue whale. At least for now, I think our models are still not powerful enough to be “dangerous” — and sure mechanistic interpretability also helps improve this.<h2 id=ai-and-politics>AI and Politics</h2><p>This is a TODO section (also too large a topic to discuss) — I’ll complete this part when time allows.<hr><ol class=footnotes-list><li id=fn-1><p>I started from an Chinese article summarizing <a href=https://www.darioamodei.com/post/on-deepseek-and-export-controls><em>On DeepSeek and Export Controls</em></a>. Then I read the original English post, and other posts from <a href=https://www.darioamodei.com/>Amodei’s homepage</a> afterwards. <a href=#fr-1-1>↩</a></p><li id=fn-2><p>These are summarized from the <a href=https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework><em>Basic assumptions and framework</em></a> section of the original post. <a href=#fr-2-1>↩</a></p><li id=fn-3><p><em>“Twenty years ago US policymakers believed that free trade with China would cause it to liberalize as it became richer; that very much didn’t happen, and we now seem headed for a second cold war with a resurgent authoritarian bloc.”</em> — In the first paragraph of <a href=https://www.darioamodei.com/essay/machines-of-loving-grace#4-peace-and-governance><em>Peace and governance</em></a> section. <a href=#fr-3-1>↩</a></p><li id=fn-4><p>Elhage, Nelson, et al. “Softmax Linear Units.” <em>Transformer Circuits Thread</em>, 27 June 2022, <a href=https://transformer-circuits.pub/2022/solu/index.html>https://transformer-circuits.pub/2022/solu/index.html</a>. <a href=#fr-4-1>↩</a></p><li id=fn-5><p>Bricken, Trenton, et al. “Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.” <em>Transformer Circuits Thread</em>, 4 Oct. 2023, <a href=https://transformer-circuits.pub/2023/monosemantic-features>https://transformer-circuits.pub/2023/monosemantic-features</a>. <a href=#fr-5-1>↩</a></p><li id=fn-6><p>Lindsey, Jack, et al. “On the Biology of a Large Language Model.” <em>Transformer Circuits Thread</em>, 27 Mar. 2025, <a href=https://transformer-circuits.pub/2025/attribution-graphs/biology.html>https://transformer-circuits.pub/2025/attribution-graphs/biology.html</a>. <a href=#fr-6-1>↩</a></p><li id=fn-7><p><a href=https://www.anthropic.com/research/mapping-mind-language-model><em>Mapping the Mind of a Large Language Model</em></a>, and <a href=https://www.anthropic.com/research/tracing-thoughts-language-model><em>Tracing the thoughts of a large language model</em></a> <a href=#fr-7-1>↩</a></p></ol></section><nav class="full-width article-navigation"><div><a aria-describedby=left_title aria-label=Prev href=https://xxxxyu.github.io/blog/posts/edge-development-cheatsheet/smartphone-setup-cheatsheet/><span class=arrow>←</span> Prev</a><p aria-hidden=true id=left_title>Cheatsheet for Setting up Android Smartphones</div><div><a aria-describedby=right_title aria-label=Next href=https://xxxxyu.github.io/blog/posts/vla-models-review/>Next <span class=arrow>→</span></a><p aria-hidden=true id=right_title>Vision-Language-Action (VLA) Models: A Review of Recent Progress</div></nav></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/#envisioning-powerful-ai>Envisioning Powerful AI</a><li><a href=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/#mechanistic-interpretability-for-safe-ai>Mechanistic Interpretability for Safe AI</a><li><a href=https://xxxxyu.github.io/blog/posts/amodei-blog-reading-notes/#ai-and-politics>AI and Politics</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://xxxxyu.github.io/blog/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://xxxxyu.github.io/blog/atom.xml> <img alt=feed loading=lazy src=https://xxxxyu.github.io/blog/social_icons/rss.svg title=feed> </a><li class=js><a class="nav-links no-hover-padding social" data-encoded-email=eGlhbmd5dS5zZGxjQGZveG1haWwuY29t href=#><img alt=email loading=lazy src=https://xxxxyu.github.io/blog/social_icons/email.svg title=email> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/xxxxyu/> <img alt=github loading=lazy src=https://xxxxyu.github.io/blog/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" href="https://scholar.google.com/citations?user=IjoWeIMAAAAJ&hl=en" rel=" me"> <img alt=scholar loading=lazy src=https://xxxxyu.github.io/blog/social_icons/scholar.svg title=scholar> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://space.bilibili.com/244315926/> <img alt=bilibili loading=lazy src=https://xxxxyu.github.io/blog/social_icons/bilibili.svg title=bilibili> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>Xyu’s Blog © 2025 Xiangyu Li • Unless otherwise noted, the content in this website is available under the <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a> license.</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> • <a href=https://github.com/xxxxyu/blog> Site source </a></small></div></section><script async src=https://xxxxyu.github.io/blog/js/decodeMail.min.js></script><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>