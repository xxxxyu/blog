<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data: 'self';img-src 'self' https://* data:;media-src 'self';style-src 'self' 'unsafe-inline';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://xxxxyu.github.io/blog/ name=base><title>
Xyu's Blog • Enhancing GPTQv2 Format Support in vLLM: Analysis and Implementation</title><link href=https://xxxxyu.github.io/blog/img/xyu_logo.png rel=icon type=image/png><link title="Xyu's Blog - Atom Feed" href=https://xxxxyu.github.io/blog/atom.xml rel=alternate type=application/atom+xml><link href="https://xxxxyu.github.io/blog/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://xxxxyu.github.io/blog/main.css?h=4fac61238e0c1fab5df1" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Deep technical analysis of GPTQv2 format limitations in vLLM, and implementation of CUDA kernel adaptations to enable efficient low-bit/asymmetric quantization inference." name=description><meta content="Deep technical analysis of GPTQv2 format limitations in vLLM, and implementation of CUDA kernel adaptations to enable efficient low-bit/asymmetric quantization inference." property=og:description><meta content="Enhancing GPTQv2 Format Support in vLLM: Analysis and Implementation" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/ property=og:url><meta content="Xyu's Blog" property=og:site_name><noscript><link href=https://xxxxyu.github.io/blog/no_js.css rel=stylesheet></noscript><script src=https://xxxxyu.github.io/blog/js/initializeTheme.min.js></script><script defer src=https://xxxxyu.github.io/blog/js/themeSwitcher.min.js></script><script src="https://xxxxyu.github.io/blog/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://xxxxyu.github.io/blog/>Xyu's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/posts/>Posts </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/archive/>Archive </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/tags/>Tags </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/poems/>Poems </a><li><a class="nav-links no-hover-padding" href=https://xxxxyu.github.io/blog/academic/>Academic </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">Enhancing GPTQv2 Format Support in vLLM: Analysis and Implementation</h1><a class="u-url u-uid" href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/></a><ul class=meta><li>By <span class=p-author>Xiangyu Li</span></li><span class="hidden p-author h-card"> <a title="Xiangyu Li" class=u-url href=https://xxxxyu.github.io/blog/ rel=author>Xiangyu Li</a> </span><li><time class=dt-published datetime=2025-10-12><span aria-hidden=true class=separator>•</span>12th Oct 2025</time><li title="2265 words"><span aria-hidden=true class=separator>•</span>12 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/vllm/>vLLM</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/development/>Development</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/quantization/>Quantization</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/gptq/>GPTQ</a>, <li class=tag><a class=p-category href=https://xxxxyu.github.io/blog/tags/llm/>LLM</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2025-10-28>Updated on 28th Oct 2025</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/xxxxyu/blog/commits/main/content/posts/vllm-gptqv2-support/index.md>See changes</a></ul><div class=toc-container><h3>Table of Contents</h3><ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#introduction>Introduction</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#background-and-preliminaries>Background and Preliminaries</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#weight-quantization-of-llms>Weight Quantization of LLMs</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#gptq-quantization-and-checkpoint-format>GPTQ: Quantization and Checkpoint Format</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#from-gptqv1-to-gptqv2>From GPTQv1 to GPTQv2</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#cause-analysis>Cause Analysis</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-kernel-routing-hierarchy>vLLM’s Kernel Routing Hierarchy</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-support-for-gptq-v2-format>vLLM’s Support for GPTQ(v2) Format</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#solution>Solution</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#approach-adapt-gptq-linear-method-kernel>Approach: Adapt GPTQ Linear Method & Kernel</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#details-of-adaption-conditioned-on-format>Details of Adaption: Conditioned on Format</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#conclusion>Conclusion</a></ul></div><p class=p-summary hidden>Deep technical analysis of GPTQv2 format limitations in vLLM, and implementation of CUDA kernel adaptations to enable efficient low-bit/asymmetric quantization inference.<section class="e-content body"><blockquote><p>Issue (closed): <a href=https://github.com/vllm-project/vllm/issues/26343>#26343</a> <br> Pull request (merged): <a href=https://github.com/vllm-project/vllm/pull/26092>#26092</a> <br> Commit: <a href=https://github.com/vllm-project/vllm/commit/5cc6bddb6ef5e8e5c10de8122a43fd6e8c1e3b4b><code>5cc6bdd</code></a></blockquote><h2 id=introduction>Introduction</h2><p>vLLM, one of the leading LLM inference frameworks, currently lacks robust support for <strong>GPTQv2 format</strong> (an upgraded version of GPTQv1 format) models, particularly those using <strong>low-bit (2/3-bit) or asymmetric quantization</strong>. While vLLM doesn’t raise explicit errors when loading such models, it incorrectly treats them as GPTQv1 format, resulting in degraded inference quality and characteristic gibberish outputs (consisting of repeated <code>!!!</code>, details in <a href=https://github.com/vllm-project/vllm/issues/26343>this issue</a>).<p>This limitation stems from differences in <strong>zero point handling</strong> between GPTQv1 and GPTQv2 checkpoint formats, which vLLM’s existing GPTQ GeMM kernels don’t account for. This post presents a comprehensive analysis of this limitation, and documents the implementation of kernel adaptations (i.e., in <a href=https://github.com/vllm-project/vllm/pull/26092>this PR</a>), that enable proper GPTQv2 support while maintaining backward compatibility.<p>Through careful investigation of vLLM’s quantization support and targeted CUDA kernel modifications, I enable robust inference for GPTQv2 format models, especially low-bit or asymmetric ones, with vLLM — contributing a step forward towards efficient LLM deployment.<h2 id=background-and-preliminaries>Background and Preliminaries</h2><p>In my case, I use vLLM to serve some low-bit (e.g., 2-bit), asymmetrically quantized models, stored in GPTQv2 format, and encountered <a href=https://github.com/vllm-project/vllm/issues/26343>this issue</a>.<p>Before diving into technical details, I’ll briefly introduce the background to do so (e.g., why GPTQv2), and some preliminaries (e.g., the checkpoint format) to help follow the technical parts. <strong>Key takeaways:</strong><ul><li><strong>Asymmetric quantization benefits low-bit quantization</strong>, by adjusting the zero point for each group of weights.<li><strong>GPTQv2 format outperforms GPTQv1 in asymmetric quantization</strong>, by better preserving zero point information.</ul><h3 id=weight-quantization-of-llms>Weight Quantization of LLMs</h3><p>Weight quantization that quantizes high-precision model weights (e.g., 16/32-bit) into fewer bits (e.g., 2/3/4/8-bit) has been a common practice in LLM deployment, especially in resource-constrained scenarios.<p>Technically, weight quantization maps the large range of high-precision weights (e.g., within $[-65504, 65504]$ for FP16) into a limited range of quantized weights (e.g., $[0, 2^b - 1]$ for $b$-bit unsigned integer). This mapping typically involves a scaling factor ($scale$) that compresses the range, and a bias ($zero$) that shifts the zero point. We denote $w_o$ as the original weight within the range $[w_{min}, w_{max}]$ (usually for a group of weights), and $w_q$ as the quantized weight. Then, a simple $b$-bit integer quantization is formulated as:<p>\[ scale = \frac{w_{max} - w_{min}}{2^b - 1} \]<p>\[ zero = - \mathrm{round}(\frac{w_{min}}{scale}) \]<p>\[ w_q = \mathrm{clamp}(\mathrm{round}(\frac{w_o}{scale}) + zero) \]<p>To recover the original weight $\hat{w}_o$ during dequantization:<p>\[ \hat{w}_o = (w_q - zero) \cdot scale \]<p>Based on this formulation, quantization methods are categorized by whether the zero point is required:<ul><li><p><em>Symmetric quantization</em> assumes $w_{max} = - w_{min}$, so $zero = - \mathrm{round}(\frac{2^b - 1}{2})$ will not change. In this case, $zero$ doesn’t provide additional information given the quantization bits.</p><li><p><em>Asymmetric quantization</em> doesn’t have such assumption. So $zero$ varies across groups of weights, and is necessary for accurately recovering the original weights.</p></ul><p>Note that most GPTQ implementations are 4-bit symmetric quantization. However, to reduce the quantization error in lower bits, asymmetric quantization is necessary.<h3 id=gptq-quantization-and-checkpoint-format>GPTQ: Quantization and Checkpoint Format</h3><p>GPTQ<sup class=footnote-reference id=fr-1-1><a href=#fn-1>[1]</a></sup> is one of the most popular post-training <strong>quantization methods</strong> for generative transformers (mainly LLMs and VLMs). It utilizes approximate second-order information (inverse layer Hessian) to reduce quantization errors. Besides, GPTQ could also refer to the specific <strong>checkpoint format</strong> adopted by GPTQ-quantized models (e.g., by <a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>), with details explained in <a href=https://danieldk.eu/GPTQ-Checkpoint-Format>this blog</a>.<p>GPTQ is widely supported by the community, including 1) quantization libraries that implement the GPTQ quantization method or support exporting to GPTQ format (although not implementing the quantization method), and 2) kernel libraries and inference frameworks that support inference with models of the GPTQ checkpoint format, as listed below:<p>Quantization libraries:<ul><li><a href=https://github.com/ModelCloud/GPTQModel>GPTQModel</a> is now the 1st choice for GPTQ quantization in replacement of AutoGPTQ, with richer model and backend support.<li><a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a> was the most popular library for GPTQ quantization, and is unmaintained now.<li><a href=https://github.com/IST-DASLab/gptq>gptq</a> is the official code implementation of the GPTQ paper.<li>Many other libraries implement GPTQ quantization, or support to save quantized models in GPTQ format.</ul><p>Computing (CUDA) kernels:<ul><li><a href=https://github.com/IST-DASLab/marlin>Marlin</a> is the SOTA W4A16 mpGeMM kernel supporting 4-bit GPTQ models.<li><a href=https://github.com/turboderp-org/exllamav2>ExLlamaV2</a> is an inference library with high-performance kernels for GPTQ models.<li><a href=https://github.com/microsoft/BitBLAS>BitBLAS</a> is a high-performance low-bit mpGeMM kernel library supporting GPTQ models.<li>Similarly, many other kernel libraries support GPTQ format models.</ul><p>SOTA LLM inference frameworks, including <a href=https://github.com/vllm-project/vllm>vLLM</a>, are integrated with the above quantization and kernel libraries, to support efficient LLM deployment with GPTQ quantization.<h3 id=from-gptqv1-to-gptqv2>From GPTQv1 to GPTQv2</h3><p>GPTQv2 is an upgraded version of GPTQ (by a different team though), in both the quantization method and checkpoint format. Specifically:<ul><li>Quantization method: GPTQv2 (also called GPTAQ) introduces asymmetric calibration, which effectively reduces the quantization error accumulated in previous layers<sup class=footnote-reference id=fr-2-1><a href=#fn-2>[2]</a></sup>. It’s first implemented in <a href=https://github.com/Intelligent-Computing-Lab-Panda/GPTAQ>GPTAQ</a>, and then integrated to <a href=https://github.com/ModelCloud/GPTQModel>GPTQModel</a> (enabled by setting <code>v2=True</code>).<li>Checkpoint format: GPTQv2 format stores the zero points differently with GPTQv1 (i.e., GPTQ) format — GPTQv1 stores $\mathrm{clamp}(zero - 1)$ in the checkpoint, and requires adding $1$ back before dequantization at runtime<sup class=footnote-reference id=fr-3-1><a href=#fn-3>[3]</a></sup>. GPTQv2 stores the exact $zero$ value, and doesn’t require extra runtime adjustment. It is also supported by <a href=https://github.com/ModelCloud/GPTQModel>GPTQModel</a> (enabled by setting <code>format="gptq_v2"</code>).</ul><p>Just like GPTQv1, the quantization method and checkpoint format are not coupled. So you can:<ul><li>Store a non-GPTQ-quantized model in GPTQv2 format, like <a href=https://huggingface.co/BitDistiller/Qwen-8B-w2g64-gptq>BitDistiller/Qwen-8B-w2g64-gptq</a>.<li>Store a GPTQv1-quantized model in GPTQv2 format, by setting <code>format="gptq_v2"</code> only, in GPTQModel.<li>Store a GPTQv2-quantized model in GPTQv1 format, by setting <code>v2=True</code> only, in GPTQModel.</ul><p>Note that the conversion between GPTQv2 and GPTQv1 format is <strong>irreversible</strong> — you can convert GPTQv1 to GPTQv2 losslessly, but not from GPTQv2 to GPTQv1. This is due to the “-1” issue of GPTQv1 as mentioned above. In this way, the actual zero point range in suppressed by clamping $0 - 1$ to $0$ in GPTQv1. For example, in INT2 quantization, the effective range shrinks from $[0,3]$ to $[1,3]$. Therefore, <strong>GPTQv2 format is a preferable choice in asymmetric quantization, especially for low-bit quantization.</strong><h2 id=cause-analysis>Cause Analysis</h2><p>After all the preparation, we can finally dive into the technical details about why and how vLLM fails for GPTQv2 in my case — even though it has some sort of support actually, which I found after careful investigation. <strong>Key takeaways:</strong><ul><li><strong>vLLM has several GPTQ-compatible GeMM kernels with pre-defined priorities</strong> — Marlin, BitBLAS, and fallbacks.<li><strong>Performant kernels like Marlin support GPTQv2 format of limited bits and symmetry</strong> — only 4/8-bit symmetric quantization.<li><strong>Fallback kernels support more bits and symmetry but lacks GPTQv2 format support</strong> — the reason why I encountered the issue.</ul><noscript><strong>⚠️ JavaScript is required to render the diagram.</strong></noscript><pre class=mermaid>
    graph TD
    A[VllmConfig] --> B[ModelConfig._verify_quantization]

    B --> |"Priority: gptq_marlin > gptq_bitblas > gptq"| E[QuantizationConfig.get_quant_method]
    
    E -->|4/8-bit + sym| J[GPTQMarlinLinearMethod]
    E -->|4/8-bit + sym| K[GPTQBitBLASLinearMethod]
    E -->|2/3/4/8-bit + sym/asym| L[GPTQLinearMethod]
    
    J --> JJ[MarlinLinearKernel]
    K --> KK[BitBLASLinearKernel]
    L --> LL[Direct Kernel Call]
    
    JJ --> M[gptq_marlin_gemm<br>CUDA kernel]
    KK --> N[bitblas.Matmul<br>External library]
    LL --> O[gptq_gemm<br>CUDA kernel]
    
    M --> Q["✅ Marlin: gptq/gptq_v2"]
    N --> R["✅ BitBLAS: gptq/gptq_v2"] 
    O --> S["❌ GPTQ: gptq only"]
    
    style J fill:#90EE90
    style K fill:#90EE90
    style L fill:#FFB6C1
    style Q fill:#90EE90
    style R fill:#90EE90
    style S fill:#FFB6C1
</pre><h3 id=vllm-s-kernel-routing-hierarchy>vLLM’s Kernel Routing Hierarchy</h3><p>The first step is to understand how vLLM routes computing kernels for different quantizations, like GPTQ. This is implemented in the model execution part of the <a href=https://docs.vllm.ai/en/stable/design/arch_overview.html#llm-engine>LLMEngine</a>. For simplicity, we only consider dense models (no MoE). It includes the following calling hierarchy:<p><strong>1. Model-level quantization configuration:</strong><ul><li>In model implementations (<code>vllm/model_executor/models</code>), <code>vllm_config: VllmConfig</code> is passed to the model at initialization, which contains <code>quant_config: QuantizationConfig</code>. <ul><li>Each quantization extends <code>QuantizationConfig</code> with quantization-specific overrides (e.g., <code>GPTQConfig</code>). See <a href=https://docs.vllm.ai/en/stable/api/vllm/model_executor/layers/quantization/index.html>all quantizations</a>.<li>If no quantization is specified in <code>quant_config</code>, <code>ModelConfig._verify_quantization</code> will select one from a priority list (see <a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-gptq-support-notes>below</a>).</ul><li>Each linear module of this model is initialized with <code>quant_config</code>.</ul><p><strong>2. Layer-level quantization configuration (linear methods):</strong><ul><li>At initialization, each quantized linear module determines <code>quant_method = quant_config.get_quant_method</code>. <ul><li><code>get_quant_method</code> returns a specific linear method class (inherited from <code>LinearMethodBase</code>, e.g., <code>GPTQLinearMethod</code>) depending on the quantization configuration.</ul><li>Each linear module calls the computing kernel by overriding <code>LinearMethodBase.apply</code> (e.g., <code>GPTQLinearMethod.apply</code> calls <code>gptq_gemm</code>).</ul><p><strong>3. (Optional) Kernel selection (linear kernels):</strong><p>vLLM supports several ways of routing to low-level CUDA kernels from a linear method class:<ul><li>Direct calling. For example, <code>GPTQLinearMethod</code> directly routes to <code>gptq_gemm</code>, a registered custom operand of vLLM (implemented in <code>csrc/quantization/gptq</code>).<li>Indirect calling. When multiple kernels are available for a linear method, or a kernel is available for multiple linear methods, vLLM supports extending the <code>MPLinearKernel</code> class as an interface for routing to this kernel (in <code>vllm/model_executor/layers/quantization/kernels/mixed_precision</code>). For example, <code>GPTQBitBLASLinearMethod</code> routes to <code>BitBLASLinearKernel</code>, and <code>GPTQMarlinLinearMethod</code> calls <code>choose_mp_linear_kernel</code> for flexible routing.<li>External calling (orthogonal to the above). vLLM supports calling kernels from external libraries. For example, <code>BitBLASLinearKernel</code> calls <code>Matmul</code> from the <code>bitblas</code> Python library.</ul><h3 id=vllm-s-support-for-gptq-v2-format>vLLM’s Support for GPTQ(v2) Format</h3><p>vLLM integrates several optimized kernels for GPTQ format models, as listed in <a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#gptq-quantization-and-checkpoint-format>GPTQ: Quantization and Checkpoint Format</a>, including Marlin, ExLlamaV2, BitBLAS, etc. vLLM also has fallback kernels for unsupported quantization configurations of these kernels. Following the analysis in <a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-kernel-routing-hierarchy>vLLM’s Kernel Routing Hierarchy</a>, I summarize this support matrix (by linear methods):<table><thead><tr><th>Method<th>Bits<th>Sym<th>GPTQ Format<tbody><tr><td>GPTQMarlin<td>4,8<td>True<td><code>gptq, gptq_v2</code><tr><td>GPTQBitBLAS<td>4,8<td>True<td><code>gptq, gptq_v2</code><tr><td>GPTQ<td>2,3,4,8<td>Any<td><code>gptq</code></table><p><a id=vllm-gptq-support-notes></a><p>Notes:<ul><li>All methods support 4/8-bit symmetric quantization. vLLM selects the most performant method supported by the current configuration, according to predefined priorities of quantization overrides (in <code>ModelConfig._verify_quantization</code>):</ul><pre class="language-py z-code" data-lang=py><code class=language-py data-lang=py><span class="z-source z-python"><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">overrides</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-sequence z-list z-python"><span class="z-punctuation z-section z-sequence z-begin z-python">[</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-constant z-language z-python">...</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-comment z-line z-number-sign z-python"><span class="z-punctuation z-definition z-comment z-python">#</span> gptq_marlin_24 requires special format,
</span></span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-comment z-line z-number-sign z-python"><span class="z-punctuation z-definition z-comment z-python">#</span> so we don't consider here.
</span></span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python"><span class="z-punctuation z-definition z-string z-begin z-python">"</span></span></span><span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python">gptq_marlin_24<span class="z-punctuation z-definition z-string z-end z-python">"</span></span></span><span class="z-punctuation z-separator z-sequence z-python">,</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-comment z-line z-number-sign z-python"><span class="z-punctuation z-definition z-comment z-python">#</span> Priority: gptq_marlin > gptq_bitblas > gptq.
</span></span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python"><span class="z-punctuation z-definition z-string z-begin z-python">"</span></span></span><span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python">gptq_marlin<span class="z-punctuation z-definition z-string z-end z-python">"</span></span></span><span class="z-punctuation z-separator z-sequence z-python">,</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python"><span class="z-punctuation z-definition z-string z-begin z-python">"</span></span></span><span class="z-meta z-string z-python"><span class="z-string z-quoted z-double z-python">gptq_bitblas<span class="z-punctuation z-definition z-string z-end z-python">"</span></span></span><span class="z-punctuation z-separator z-sequence z-python">,</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">                <span class="z-constant z-language z-python">...</span>
</span></span><span class="z-source z-python"><span class="z-meta z-sequence z-list z-python">            <span class="z-punctuation z-section z-sequence z-end z-python">]</span></span>
</span></code></pre><ul><li>Only <code>GPTQLinearMethod</code> supports 2/3-bit quantization and asymmetric quantization. However, it lacks GPTQv2 support (both the other two supports).</ul><p>As a result, neither 2/3-bit nor asymmetric quantization in GPTQv2 format are unsupported by vLLM, which motivates <a href=https://github.com/vllm-project/vllm/pull/26092>this PR</a>.<h2 id=solution>Solution</h2><p>Based on the above analysis, vLLM’s GPTQ linear methods lack support for 2/3-bit quantization and asymmetric quantization in GPTQv2 format, and require adaption to robustly support GPTQv2 format models of various configurations.<h3 id=approach-adapt-gptq-linear-method-kernel>Approach: Adapt GPTQ Linear Method & Kernel</h3><p>To add such support, at least one linear method should be added/adapted. To adapt existing linear methods:<ul><li><code>GPTQMarlinLinearMethod</code> (lacking 2/3-bit and asymmetric support): It requires also modifying vLLM’s Marlin CUDA kernel, which is dedicated for 4/8-bit symmetric quantization — not a good choice.<li><code>GPTQBitBLASLinearMethod</code> (lacking 2/3-bit and asymmetric support): It requires modifying only the linear method/kernel (Python code), since the <code>bitblas</code> library itself supports the <code>bits</code> and <code>sym</code> we want — a reasonable choice, but requires the optional <code>bitblas</code> package to be installed.<li><code>GPTQLinearMethod</code> (lacking GPTQv2 format support): It requires also modifying vLLM’s <code>gptq_gemm</code> CUDA kernel, by only adapting the zero point handling logic — preferred.</ul><p>So, the plan is to adapt <code>GPTQLinearMethod</code> (with <code>GPTQConfig</code>) and <code>gptq_gemm</code> to add proper GPTQv2 format support.<h3 id=details-of-adaption-conditioned-on-format>Details of Adaption: Conditioned on Format</h3><p>During this linear method & kernel adaption, there are 3 points to keep in mind:<ol><li>Maintain compatibility for GPTQv1 format.<li>Keep the code and binary size impact down.<li>Make sure other kernels (e.g., Marlin) are not running with GPTQv2 incorrectly.</ol><p>In response to Pt. 1 and 2:<ul><li>Add a <code>use_v2_format: bool</code> attribute to <code>GPTQLinearMethod</code> that indicates whether <code>checkpoint_format == "gptq_v2"</code>.<li>Add a <code>bool use_v2_format</code> argument to <code>gptq_gemm</code>, which accepts <code>GPTQLinearMethod.use_v2_format</code> as input.<li>In <code>gptq_gemm</code>, update the zero point handling logic to be conditioned on <code>use_v2_format</code>. For example:</ul><pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-source z-c"><span class="z-comment z-line z-double-slash z-c"><span class="z-punctuation z-definition z-comment z-c">//</span> In `reconstruct_exllama_2bit_kernel`:
</span></span><span class="z-source z-c">
</span><span class="z-source z-c"><span class="z-comment z-line z-double-slash z-c"><span class="z-punctuation z-definition z-comment z-c">//</span> Previous: zeros[i] + 1 (hardcoded for GPTQv1)
</span></span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">x</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">0</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">0</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> <span class="z-constant z-numeric z-integer z-decimal z-c">1</span></span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">y</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">1</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">1</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> <span class="z-constant z-numeric z-integer z-decimal z-c">1</span></span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">z</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">2</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">2</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> <span class="z-constant z-numeric z-integer z-decimal z-c">1</span></span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">w</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">3</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">3</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> <span class="z-constant z-numeric z-integer z-decimal z-c">1</span></span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c">
</span><span class="z-source z-c"><span class="z-comment z-line z-double-slash z-c"><span class="z-punctuation z-definition z-comment z-c">//</span> Now: zeros[i] + offset (conditioned on `use_v2_format`)
</span></span><span class="z-source z-c"><span class="z-storage z-type z-c">int</span> zero_offset <span class="z-keyword z-operator z-assignment z-c">=</span> use_v2_format <span class="z-keyword z-operator z-ternary z-c">?</span> <span class="z-constant z-numeric z-integer z-decimal z-c">0</span> <span class="z-keyword z-operator z-ternary z-c">:</span> <span class="z-constant z-numeric z-integer z-decimal z-c">1</span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-keyword z-operator z-variadic z-c">...</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">x</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">0</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">0</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> zero_offset</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">y</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">1</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">1</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> zero_offset</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">z</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">2</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">2</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> zero_offset</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c"><span class="z-meta z-function-call z-c"><span class="z-variable z-function z-c">dequant_2bit_16</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-begin z-c">(</span></span></span><span class="z-meta z-function-call z-c"><span class="z-meta z-group z-c">load_int4<span class="z-punctuation z-accessor z-c">.</span><span class="z-variable z-other z-member z-c">w</span><span class="z-punctuation z-separator z-c">,</span> dq<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">3</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span><span class="z-punctuation z-separator z-c">,</span> size_n<span class="z-punctuation z-separator z-c">,</span> zeros<span class="z-meta z-brackets z-c"><span class="z-punctuation z-section z-brackets z-begin z-c">[</span><span class="z-constant z-numeric z-integer z-decimal z-c">3</span><span class="z-punctuation z-section z-brackets z-end z-c">]</span></span> <span class="z-keyword z-operator z-arithmetic z-c">+</span> zero_offset</span><span class="z-meta z-group z-c"><span class="z-punctuation z-section z-group z-end z-c">)</span></span></span><span class="z-punctuation z-terminator z-c">;</span>
</span><span class="z-source z-c">
</span></code></pre><blockquote><p>When testing, I found that the original <code>gptq_gemm</code> is buggy at 4-bit even with symmetrically quantized model of GPTQv1 format — out of scope of this PR.</blockquote><p>To ensure Pt. 3, review <a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-support-for-gptq-v2-format>vLLM’s Support for GPTQ(v2) Format</a> — both <code>GPTQMarlinLinearMethod</code> and <code>GPTQBitBLASLinearMethod</code> are not affected, as they already support GPTQv2 format, though limited to 4/8-bit symmetric quantization.<blockquote><p>TODO: Add some performance benchmarks. Currently I’ve found that 2-bit gptq_gemm is slower during decoding (GeMV) than prefilling (GeMM).</blockquote><h2 id=conclusion>Conclusion</h2><p>This post details the development of GPTQv2 format support in vLLM, which addresses a significant gap in low-bit asymmetric quantization inference with SOTA LLM inference frameworks.<p>Questions and discussions are welcomed.<p><strong>Possible future works:</strong><ul><li>Extend optimized kernels (Marlin, BitBLAS) to support 2/3-bit or asymmetric quantization.<li>Fix the 4-bit bug in <code>gptq_gemm</code>.<li>Improve the decoding speed with <code>gptq_gemm</code>.</ul><hr><ol class=footnotes-list><li id=fn-1><p>Frantar, Elias, et al. “GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers.” arXiv preprint arXiv:2210.17323 (2022). <a href=#fr-1-1>↩</a></p><li id=fn-2><p>Li, Yuhang, et al. “GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration.” arXiv preprint arXiv:2504.02692 (2025). <a href=#fr-2-1>↩</a></p><li id=fn-3><p>de Kok, Daniël. “GPTQ Checkpoint Format.” Daniël’s Website, 7 Aug. 2024, danieldk.eu/GPTQ-Checkpoint-Format. <a href=#fr-3-1>↩</a></p></ol></section><nav class="full-width article-navigation"><div><a aria-describedby=left_title aria-label=Prev href=https://xxxxyu.github.io/blog/posts/vla-models-review/><span class=arrow>←</span> Prev</a><p aria-hidden=true id=left_title>Vision-Language-Action (VLA) Models: A Review of Recent Progress</div><div></div></nav></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#introduction>Introduction</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#background-and-preliminaries>Background and Preliminaries</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#weight-quantization-of-llms>Weight Quantization of LLMs</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#gptq-quantization-and-checkpoint-format>GPTQ: Quantization and Checkpoint Format</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#from-gptqv1-to-gptqv2>From GPTQv1 to GPTQv2</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#cause-analysis>Cause Analysis</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-kernel-routing-hierarchy>vLLM’s Kernel Routing Hierarchy</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#vllm-s-support-for-gptq-v2-format>vLLM’s Support for GPTQ(v2) Format</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#solution>Solution</a> <ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#approach-adapt-gptq-linear-method-kernel>Approach: Adapt GPTQ Linear Method & Kernel</a><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#details-of-adaption-conditioned-on-format>Details of Adaption: Conditioned on Format</a></ul><li><a href=https://xxxxyu.github.io/blog/posts/vllm-gptqv2-support/#conclusion>Conclusion</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://xxxxyu.github.io/blog/katex.min.css rel=stylesheet><script defer src=https://xxxxyu.github.io/blog/js/katex.min.js></script><script defer src=https://xxxxyu.github.io/blog/js/mermaid.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://xxxxyu.github.io/blog/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://xxxxyu.github.io/blog/atom.xml> <img alt=feed loading=lazy src=https://xxxxyu.github.io/blog/social_icons/rss.svg title=feed> </a><li class=js><a class="nav-links no-hover-padding social" data-encoded-email=eGlhbmd5dS5zZGxjQGZveG1haWwuY29t href=#><img alt=email loading=lazy src=https://xxxxyu.github.io/blog/social_icons/email.svg title=email> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/xxxxyu/> <img alt=github loading=lazy src=https://xxxxyu.github.io/blog/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" href="https://scholar.google.com/citations?user=IjoWeIMAAAAJ&hl=en" rel=" me"> <img alt=scholar loading=lazy src=https://xxxxyu.github.io/blog/social_icons/scholar.svg title=scholar> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://space.bilibili.com/244315926/> <img alt=bilibili loading=lazy src=https://xxxxyu.github.io/blog/social_icons/bilibili.svg title=bilibili> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>Xyu’s Blog © 2025 Xiangyu Li • Unless otherwise noted, the content in this website is available under the <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a> license.</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> • <a href=https://github.com/xxxxyu/blog> Site source </a></small></div></section><script async src=https://xxxxyu.github.io/blog/js/decodeMail.min.js></script><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>